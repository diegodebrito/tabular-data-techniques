{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: https://www.kaggle.com/cdeotte/xgb-fraud-with-magic-0-9600"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Packages and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-19T15:13:38.726417Z",
     "start_time": "2020-03-19T15:13:36.333267Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-19T20:00:33.382240Z",
     "start_time": "2020-03-19T20:00:33.377277Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-19T15:13:38.879423Z",
     "start_time": "2020-03-19T15:13:38.844298Z"
    }
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-19T15:13:39.311877Z",
     "start_time": "2020-03-19T15:13:38.881299Z"
    }
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-20T01:25:50.654476Z",
     "start_time": "2020-03-20T01:25:50.648477Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_uids(dfs, groups):\n",
    "    \"\"\"\n",
    "    Cretes uids based on lists of variables given on groups\n",
    "    It can be used on a list of dataframes given by dfs\n",
    "    \n",
    "    \"\"\"\n",
    "    for df in dfs:\n",
    "        created_columns = []\n",
    "        for group in groups:\n",
    "            new_name = '_'.join(group)\n",
    "            df[new_name] = ''\n",
    "            for col in group:\n",
    "                df[new_name] += df[col].astype(str)\n",
    "            df[new_name] = df[new_name].astype('category')\n",
    "            created_columns.append(new_name)\n",
    "    \n",
    "    return dfs, created_columns\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-20T01:30:19.641708Z",
     "start_time": "2020-03-20T01:30:19.634715Z"
    }
   },
   "outputs": [],
   "source": [
    "def freq_encode_full(train, test, cols):\n",
    "    \n",
    "    \"\"\"\n",
    "    Frequency encoding for variables in cols, using BOTH train\n",
    "    and test set. Appends _FULL_FE to the created variables names.\n",
    "    \n",
    "    \"\"\"\n",
    "    new_names = []\n",
    "    \n",
    "    for col in cols:\n",
    "        \n",
    "        df = pd.concat([train[[col]], test[[col]]])\n",
    "        values_dict = df[col].value_counts().to_dict()\n",
    "        train[col+'_FULL_FE'] = train[col].map(values_dict)\n",
    "        test[col+'_FULL_FE'] = test[col].map(values_dict)\n",
    "        \n",
    "        new_names.append(col+'_FULL_FE')\n",
    "        \n",
    "    return train, test, new_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-20T01:31:13.249831Z",
     "start_time": "2020-03-20T01:31:13.242830Z"
    }
   },
   "outputs": [],
   "source": [
    "def uid_aggregation_full(train, test, uids, cols, aggs):\n",
    "          \n",
    "    \"\"\"\n",
    "    Creates aggregation of features in cols, based on groups in uids,\n",
    "    using aggregations in aggs. Example: creates a new feature that \n",
    "    is the mean of C1 on groups of card1_card2.\n",
    "    \n",
    "    \"\"\"\n",
    "    for uid in uids:\n",
    "        for col in cols:\n",
    "            for agg in aggs:\n",
    "                \n",
    "                temp = pd.concat([train[[uid,col]], test[[uid,col]]])\n",
    "                new_name = uid + '_' + col + '_' + agg\n",
    "                temp = temp.groupby(uid)[col].agg([agg]).reset_index().rename(columns={agg:new_name})\n",
    "                temp.index = list(temp[uid])\n",
    "                temp = temp[new_name].to_dict()\n",
    "                \n",
    "                train[new_name] = train[uid].map(temp)\n",
    "                test[new_name] = test[uid].map(temp)\n",
    "                \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-20T01:38:27.167777Z",
     "start_time": "2020-03-20T01:38:27.163774Z"
    }
   },
   "outputs": [],
   "source": [
    "def factorize_dfs(train, test, cols):\n",
    "    \"\"\"\n",
    "    Factorize categorical columns for train and test data.\n",
    "    Mostly for XGBoost model.\n",
    "    \n",
    "    \"\"\"\n",
    "    train_size = train.shape[0]\n",
    "    \n",
    "    for col in cols:\n",
    "        df = pd.concat([train[col], test[col]])\n",
    "        df, _ = df.factorize(sort=True)\n",
    "        \n",
    "        train[col] = df[:train_size]\n",
    "        test[col] = df[train_size:]\n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-19T15:13:46.385074Z",
     "start_time": "2020-03-19T15:13:46.379074Z"
    }
   },
   "outputs": [],
   "source": [
    "def fill_nas(dfs):\n",
    "    \"\"\"\n",
    "    Fills categorical NAs with 'Missing' and numerical with -999\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Categorical features\n",
    "    cat_columns = [col for col in dfs[0].columns if dfs[0][col].dtype == object]\n",
    "    for df in dfs:\n",
    "        df[cat_columns] = df[cat_columns].fillna('Missing')\n",
    "    \n",
    "    # C columns\n",
    "    # C_columns = ['C'+str(i) for i in range(1, 15)]\n",
    "    num_columns = [col for col in dfs[0].columns if dfs[0][col].dtype != object]\n",
    "    for df in dfs:\n",
    "        #df[c_columns] = df[c_columns].fillna(0)\n",
    "        df[num_columns] = df[num_columns].fillna(-999)\n",
    "        \n",
    "    return dfs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-19T15:41:35.300926Z",
     "start_time": "2020-03-19T15:41:35.289958Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_features(dfs):\n",
    "    \n",
    "    \"\"\"\n",
    "    Creates some features based on dates, normalize columns D,\n",
    "    and creates the feature Transaction_cents\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    for df in dfs:\n",
    "        df['Day'] = df['TransactionDT'].dt.day\n",
    "        df['Day_Week'] = df['TransactionDT'].dt.dayofweek\n",
    "        df['Hour'] = df['TransactionDT'].dt.hour\n",
    "        df['Minute'] = df['TransactionDT'].dt.minute\n",
    "        \n",
    "        # Number of days\n",
    "        first_date = pd.date_range('2017-12-01', periods=1)[0]\n",
    "        df['days_from_start'] = (df['TransactionDT'] - first_date).dt.days\n",
    "        \n",
    "        # Normalizing D features\n",
    "        d_columns = ['D' + str(i) for i in range(1, 16)]\n",
    "        for col in d_columns:\n",
    "            df[col+'_n'] = df['days_from_start'] - df[col]\n",
    "        \n",
    "        df.drop(d_columns, axis=1, inplace=True)\n",
    "        \n",
    "        # Cents from TransactionAmt\n",
    "        df['Transaction_cents'] = (df['TransactionAmt'] - np.floor(df['TransactionAmt'])).astype('float32')\n",
    "        \n",
    "        # TransactionDT is no longer necessary\n",
    "        df.drop(['TransactionDT', 'days_from_start'], axis=1, inplace=True)\n",
    "    \n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-19T15:13:46.405074Z",
     "start_time": "2020-03-19T15:13:46.395072Z"
    }
   },
   "outputs": [],
   "source": [
    "def text_features(dfs):\n",
    "    \n",
    "    \"\"\"\n",
    "    Features based on text information\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    for df in dfs:\n",
    "        \n",
    "        # P_emaildomain\n",
    "        df['P_emaildomain_initial'] = df['P_emaildomain'].apply(lambda x: x.split('.')[0])\n",
    "        df['P_emaildomain_final'] = df['P_emaildomain'].apply(lambda x: '#'+''.join(x.split('.')[1:]))\n",
    "        # R_emaildomain\n",
    "        df['R_emaildomain_initial'] = df['R_emaildomain'].apply(lambda x: x.split('.')[0])\n",
    "        df['R_emaildomain_final'] = df['R_emaildomain'].apply(lambda x: '#'+''.join(x.split('.')[1:]))\n",
    "        \n",
    "        # DeviceInfo\n",
    "        temp = df['DeviceInfo'].apply(lambda x: ''.join([i for i in x if i.isalpha()]))\n",
    "        temp = temp.str.replace(r'SAMSUNGSM', 'SM')\n",
    "        df['DeviceInfo_brand'] = temp.apply(lambda x: x.split('Build')[0])\n",
    "        df['DeviceInfo_build'] = temp.apply(lambda x:'#' + ''.join(x.split('Build')[1:]))\n",
    "        \n",
    "        # id_30\n",
    "        df['id_30'] = df['id_30'].str.lower()\n",
    "        df['id_30_device'] = df['id_30'].apply(lambda x: ''.join([s for s in x if s.isalpha()]))\n",
    "        df['id_30_version'] = df['id_30'].apply(lambda x:'#'+ ''.join([s for s in x if s.isnumeric()]))\n",
    "        \n",
    "        # id_31\n",
    "        df['id_31'] = df['id_31'].str.lower()\n",
    "        df['id_31_device'] = df['id_31'].apply(lambda x: ''.join([s for s in x if s.isalpha()]))\n",
    "    \n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-19T15:43:33.958347Z",
     "start_time": "2020-03-19T15:43:33.932879Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_full(train, test, features_remove=None):\n",
    "    \n",
    "    \"\"\"\n",
    "    Preprocess the trian and validation/test dataset, creating uids\n",
    "    and aggregations. The 'magic' features are defined here.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    if features_remove is None:\n",
    "        features_remove = []\n",
    "    \n",
    "    # Remove NAs\n",
    "    train, test = fill_nas([train, test])\n",
    "    \n",
    "    # Extract features\n",
    "    train, test = extract_features([train, test])\n",
    "    \n",
    "    # Extract text features\n",
    "    train, test = text_features([train, test])\n",
    "    \n",
    "    # Creating New Feature for aggregation\n",
    "    [train, test], new_cols = create_uids([train, test],\n",
    "                                        [['card1', 'addr1'],\n",
    "                                         ['card1', 'addr1', 'P_emaildomain'],\n",
    "                                         ['card1', 'addr1', 'D1_n']\n",
    "                                        ])\n",
    "    \n",
    "    # Aggregations (no magic feature)\n",
    "    uids = ['card1', 'card1_addr1', 'card1_addr1_P_emaildomain']\n",
    "    cols = ['TransactionAmt', 'D9_n', 'D11_n']\n",
    "    aggs = ['mean', 'std']\n",
    "    train, test = uid_aggregation_full(train, test, uids, cols, aggs)\n",
    "    \n",
    "    # Aggregations with magic feature\n",
    "    uids = ['card1_addr1_D1_n']\n",
    "    cols = ['TransactionAmt','D4_n','D9_n','D10_n','D15_n']\n",
    "    aggs = ['mean', 'std']\n",
    "    train, test = uid_aggregation_full(train, test, uids, cols, aggs)\n",
    "    \n",
    "    # Aggregations with magic features (C columns)\n",
    "    uids = ['card1_addr1_D1_n']\n",
    "    cols = ['C'+str(x) for x in range(1,15) if x!=3]\n",
    "    aggs = ['mean']\n",
    "    train, test = uid_aggregation_full(train, test, uids, cols, aggs)\n",
    "    \n",
    "    # Aggregations with magic features\n",
    "    uids = ['card1_addr1_D1_n']\n",
    "    cols = ['P_emaildomain','dist1','id_02','Transaction_cents']\n",
    "    aggs = ['nunique']\n",
    "    train, test = uid_aggregation_full(train, test, uids, cols, aggs)\n",
    "    \n",
    "    # Aggregations with magic features\n",
    "    uids = ['card1_addr1_D1_n']\n",
    "    cols = ['C14']\n",
    "    aggs = ['std']\n",
    "    train, test = uid_aggregation_full(train, test, uids, cols, aggs)\n",
    "    \n",
    "    # Aggregations with magic features \n",
    "    uids = ['card1_addr1_D1_n']\n",
    "    cols = ['C13', 'V314']\n",
    "    aggs = ['mean']\n",
    "    train, test = uid_aggregation_full(train, test, uids, cols, aggs)\n",
    "    \n",
    "    # Aggregations with magic features\n",
    "    uids = ['card1_addr1_D1_n']\n",
    "    cols = ['V127','V136','V309','V307','V320']\n",
    "    aggs = ['nunique']\n",
    "    train, test = uid_aggregation_full(train, test, uids, cols, aggs)\n",
    "    \n",
    "    # Frequency Encoding\n",
    "    columns = ['addr1', 'card1', 'card2', 'card3', 'P_emaildomain',\n",
    "               'card1_addr1', 'card1_addr1_P_emaildomain',\n",
    "               'card1_addr1_D1_n']\n",
    "    train, test, new_names = freq_encode_full(train, test, columns)\n",
    "    \n",
    "    # Using frequency encoding for categoricals\n",
    "    cat_columns = [col for col in train.columns if train[col].dtype == object]\n",
    "    train[cat_columns] = train[cat_columns].astype('category')\n",
    "    test[cat_columns] = test[cat_columns].astype('category')\n",
    "    \n",
    "    return train.drop(features_remove, axis=1), test.drop(features_remove, axis=1), new_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-19T15:13:46.458070Z",
     "start_time": "2020-03-19T15:13:46.449073Z"
    }
   },
   "outputs": [],
   "source": [
    "def fit_baseline_model(X_train, X_val, y_train, y_val):\n",
    "    \n",
    "    lgb_params = {\n",
    "                    'objective':'binary',\n",
    "                    'boosting_type':'gbdt',\n",
    "                    'metric':'auc',\n",
    "                    'n_jobs':-1,\n",
    "                    'learning_rate':0.01,\n",
    "                    'num_leaves': 2**8,\n",
    "                    'max_depth':-1,\n",
    "                    'tree_learner':'serial',\n",
    "                    'colsample_bytree': 0.7,\n",
    "                    'subsample_freq':1,\n",
    "                    'subsample':0.7,\n",
    "                    'num_boosting_rounds':80000,\n",
    "                    'max_bin':255,\n",
    "                    'verbose':-1,\n",
    "                    'seed': 357,\n",
    "                    'early_stopping_rounds':100, \n",
    "                } \n",
    "    \n",
    "    lgb_train = lgb.Dataset(X_train, y_train)\n",
    "    lgb_val = lgb.Dataset(X_val, y_val)\n",
    "    \n",
    "    estimator = lgb.train(\n",
    "            lgb_params,\n",
    "            lgb_train,\n",
    "            valid_sets = [lgb_train, lgb_val],\n",
    "            verbose_eval = 200,\n",
    "        )   \n",
    "    \n",
    "    columns = X_train.columns\n",
    "    feature_imp = pd.DataFrame((zip(estimator.feature_importance(),columns)), \n",
    "                                columns=['Value','Feature'])\n",
    "    feature_imp = feature_imp.sort_values(by='Value', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    return feature_imp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data and Clean NAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-19T15:13:56.427192Z",
     "start_time": "2020-03-19T15:13:49.256719Z"
    }
   },
   "outputs": [],
   "source": [
    "# Train and Validation Sets\n",
    "train = pd.read_pickle('./data/train_pickle.pkl')\n",
    "val = train.loc[train['TransactionDT']>= '2018-05-01']\n",
    "train = train.loc[train['TransactionDT']< '2018-05-01']\n",
    "y_train = train['isFraud']\n",
    "y_val = val['isFraud']\n",
    "train.drop(['isFraud', 'TransactionID'], axis=1, inplace=True)\n",
    "val.drop(['isFraud', 'TransactionID'], axis=1, inplace=True)\n",
    "\n",
    "# Test Set\n",
    "test = pd.read_pickle('./data/test_pickle.pkl')\n",
    "test.drop(['TransactionID'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T20:02:16.141116Z",
     "start_time": "2020-04-01T20:02:16.130118Z"
    }
   },
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/cdeotte/xgb-fraud-with-magic-0-9600\n",
    "\n",
    "v =  [1, 3, 4, 6, 8, 11]\n",
    "v += [13, 14, 17, 20, 23, 26, 27, 30]\n",
    "v += [36, 37, 40, 41, 44, 47, 48]\n",
    "v += [54, 56, 59, 62, 65, 67, 68, 70]\n",
    "v += [76, 78, 80, 82, 86, 88, 89, 91]\n",
    "\n",
    "#v += [96, 98, 99, 104] #relates to groups, no NAN \n",
    "v += [107, 108, 111, 115, 117, 120, 121, 123] # maybe group, no NAN\n",
    "v += [124, 127, 129, 130, 136] # relates to groups, no NAN\n",
    "\n",
    "# LOTS OF NAN BELOW\n",
    "v += [138, 139, 142, 147, 156, 162] #b1\n",
    "v += [165, 160, 166] #b1\n",
    "v += [178, 176, 173, 182] #b2\n",
    "v += [187, 203, 205, 207, 215] #b2\n",
    "v += [169, 171, 175, 180, 185, 188, 198, 210, 209] #b2\n",
    "v += [218, 223, 224, 226, 228, 229, 235] #b3\n",
    "v += [240, 258, 257, 253, 252, 260, 261] #b3\n",
    "v += [264, 266, 267, 274, 277] #b3\n",
    "v += [220, 221, 234, 238, 250, 271] #b3\n",
    "\n",
    "v += [294, 284, 285, 286, 291, 297] # relates to grous, no NAN\n",
    "v += [303, 305, 307, 309, 310, 320] # relates to groups, no NAN\n",
    "v += [281, 283, 289, 296, 301, 314] # relates to groups, no NAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-19T02:03:28.357260Z",
     "start_time": "2020-03-19T02:03:28.352260Z"
    }
   },
   "outputs": [],
   "source": [
    "v_columns = ['V' + str(i) for i in range(1, 340) if i not in v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-19T02:05:48.255148Z",
     "start_time": "2020-03-19T02:03:28.359260Z"
    }
   },
   "outputs": [],
   "source": [
    "train, val, cat_columns = preprocess_full(train, val, features_remove=v_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-19T02:06:20.837517Z",
     "start_time": "2020-03-19T02:06:20.830527Z"
    }
   },
   "outputs": [],
   "source": [
    "lgb_params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'auc',\n",
    "    #'metric': 'None',\n",
    "    'learning_rate': 0.01,\n",
    "    'num_leaves': 2**8,\n",
    "    'max_bin': 255,\n",
    "    'max_depth': -1,\n",
    "    'bagging_freq': 5,\n",
    "    'bagging_fraction': 0.7,\n",
    "    'bagging_seed': 32,\n",
    "    'feature_fraction': 0.7,\n",
    "    'feature_fraction_seed': 32,\n",
    "    'first_metric_only': True,\n",
    "    'verbose': 100,\n",
    "    'n_jobs': -1,\n",
    "    'seed': 35,\n",
    "} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-19T02:06:20.922482Z",
     "start_time": "2020-03-19T02:06:20.840481Z"
    }
   },
   "outputs": [],
   "source": [
    "lgb_train = lgb.Dataset(train, y_train)\n",
    "lgb_val = lgb.Dataset(val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-19T02:15:13.312712Z",
     "start_time": "2020-03-19T02:06:20.924515Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 500 rounds\n",
      "[200]\ttraining's auc: 0.992799\tvalid_1's auc: 0.936985\n",
      "[400]\ttraining's auc: 0.99869\tvalid_1's auc: 0.939169\n",
      "[600]\ttraining's auc: 0.999786\tvalid_1's auc: 0.938026\n",
      "[800]\ttraining's auc: 0.999967\tvalid_1's auc: 0.938328\n",
      "Early stopping, best iteration is:\n",
      "[305]\ttraining's auc: 0.997094\tvalid_1's auc: 0.940073\n",
      "Evaluated only: auc\n"
     ]
    }
   ],
   "source": [
    "clf = lgb.train(lgb_params, lgb_train, 10000, \n",
    "                valid_sets = [lgb_train, lgb_val], \n",
    "                early_stopping_rounds=500, verbose_eval=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-19T02:15:29.613847Z",
     "start_time": "2020-03-19T02:15:29.609879Z"
    }
   },
   "outputs": [],
   "source": [
    "best_iteration = clf.best_iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submitting without retraining:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-19T02:15:42.750958Z",
     "start_time": "2020-03-19T02:15:34.096387Z"
    }
   },
   "outputs": [],
   "source": [
    "# Train and Validation Sets\n",
    "train = pd.read_pickle('./data/train_pickle.pkl')\n",
    "val = train.loc[train['TransactionDT']>= '2018-05-01']\n",
    "train = train.loc[train['TransactionDT']< '2018-05-01']\n",
    "y_train = train['isFraud']\n",
    "y_val = val['isFraud']\n",
    "train.drop(['isFraud', 'TransactionID'], axis=1, inplace=True)\n",
    "val.drop(['isFraud', 'TransactionID'], axis=1, inplace=True)\n",
    "\n",
    "# Test Set\n",
    "test = pd.read_pickle('./data/test_pickle.pkl')\n",
    "test.drop(['TransactionID'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-19T02:19:24.318724Z",
     "start_time": "2020-03-19T02:15:42.752959Z"
    }
   },
   "outputs": [],
   "source": [
    "train, test, cat_columns = preprocess_full(train, test, features_remove=v_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-19T02:19:49.586929Z",
     "start_time": "2020-03-19T02:19:24.320727Z"
    }
   },
   "outputs": [],
   "source": [
    "y_test_pred = clf.predict(test, best_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-19T02:19:49.742736Z",
     "start_time": "2020-03-19T02:19:49.588956Z"
    }
   },
   "outputs": [],
   "source": [
    "sample = pd.read_csv('./data/sample_submission.csv')\n",
    "submission = sample\n",
    "submission['isFraud'] = y_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-19T02:19:52.251491Z",
     "start_time": "2020-03-19T02:19:49.744699Z"
    }
   },
   "outputs": [],
   "source": [
    "submission.to_csv('./data/submissions/lgb_pre_udi.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying XGBoost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-19T15:43:57.978524Z",
     "start_time": "2020-03-19T15:43:45.145251Z"
    }
   },
   "outputs": [],
   "source": [
    "# Train and Validation Sets\n",
    "train = pd.read_pickle('./data/train_pickle.pkl')\n",
    "val = train.loc[train['TransactionDT']>= '2018-05-01']\n",
    "train = train.loc[train['TransactionDT']< '2018-05-01']\n",
    "y_train = train['isFraud']\n",
    "y_val = val['isFraud']\n",
    "train.drop(['isFraud', 'TransactionID'], axis=1, inplace=True)\n",
    "val.drop(['isFraud', 'TransactionID'], axis=1, inplace=True)\n",
    "\n",
    "# Test Set\n",
    "test = pd.read_pickle('./data/test_pickle.pkl')\n",
    "test.drop(['TransactionID'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-19T15:44:00.247238Z",
     "start_time": "2020-03-19T15:44:00.243238Z"
    }
   },
   "outputs": [],
   "source": [
    "v_columns = ['V' + str(i) for i in range(1, 340) if i not in v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-19T15:46:16.425211Z",
     "start_time": "2020-03-19T15:44:01.396895Z"
    }
   },
   "outputs": [],
   "source": [
    "train, val, cat_columns = preprocess_full(train, val, features_remove=v_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost does not deal with categorical features the same way the LightGBM does, so we need to encode them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-19T15:46:21.412693Z",
     "start_time": "2020-03-19T15:46:21.401695Z"
    }
   },
   "outputs": [],
   "source": [
    "cat_columns = [col for col in train.columns if train[col].dtype == object]\n",
    "to_encode = cat_columns + list(train.columns[train.dtypes == 'category'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-19T15:46:33.266983Z",
     "start_time": "2020-03-19T15:46:33.259141Z"
    }
   },
   "source": [
    "We need to factorize the categorical variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-19T15:46:46.931892Z",
     "start_time": "2020-03-19T15:46:46.011289Z"
    }
   },
   "outputs": [],
   "source": [
    "train, val = factorize_dfs(train, val, to_encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-19T15:46:48.879638Z",
     "start_time": "2020-03-19T15:46:48.873677Z"
    }
   },
   "outputs": [],
   "source": [
    "clf = xgb.XGBClassifier( \n",
    "        n_estimators=2000,\n",
    "        max_depth=12, \n",
    "        learning_rate=0.02, \n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.4, \n",
    "        missing=-1, \n",
    "        eval_metric='auc',\n",
    "        nthread=4,\n",
    "        tree_method='hist' \n",
    "        #tree_method='gpu_hist' \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-19T16:02:39.498950Z",
     "start_time": "2020-03-19T15:46:50.067190Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-auc:0.84143\n",
      "Will train until validation_0-auc hasn't improved in 100 rounds.\n",
      "[50]\tvalidation_0-auc:0.89667\n",
      "[100]\tvalidation_0-auc:0.91109\n",
      "[150]\tvalidation_0-auc:0.92605\n",
      "[200]\tvalidation_0-auc:0.93690\n",
      "[250]\tvalidation_0-auc:0.94354\n",
      "[300]\tvalidation_0-auc:0.94697\n",
      "[350]\tvalidation_0-auc:0.94902\n",
      "[400]\tvalidation_0-auc:0.95018\n",
      "[450]\tvalidation_0-auc:0.95070\n",
      "[500]\tvalidation_0-auc:0.95078\n",
      "[550]\tvalidation_0-auc:0.95094\n",
      "[600]\tvalidation_0-auc:0.95084\n",
      "Stopping. Best iteration:\n",
      "[548]\tvalidation_0-auc:0.95101\n",
      "\n"
     ]
    }
   ],
   "source": [
    "h = clf.fit(train, y_train, \n",
    "        eval_set=[(val, y_val)],\n",
    "        verbose=50, early_stopping_rounds=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-19T16:57:39.648348Z",
     "start_time": "2020-03-19T16:57:28.170935Z"
    }
   },
   "outputs": [],
   "source": [
    "# Train and Validation Sets\n",
    "train = pd.read_pickle('./data/train_pickle.pkl')\n",
    "y_train = train['isFraud']\n",
    "train.drop(['isFraud', 'TransactionID'], axis=1, inplace=True)\n",
    "\n",
    "# Test Set\n",
    "test = pd.read_pickle('./data/test_pickle.pkl')\n",
    "test.drop(['TransactionID'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-19T17:01:50.891606Z",
     "start_time": "2020-03-19T16:57:40.379356Z"
    }
   },
   "outputs": [],
   "source": [
    "train, test, cat_columns = preprocess_full(train, test, features_remove=v_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-19T17:02:28.456002Z",
     "start_time": "2020-03-19T17:02:24.631383Z"
    }
   },
   "outputs": [],
   "source": [
    "#train.to_pickle('./data/train_processed.pkl')\n",
    "#test.to_pickle('./data/test_processed.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-19T17:02:52.862245Z",
     "start_time": "2020-03-19T17:02:48.547171Z"
    }
   },
   "outputs": [],
   "source": [
    "train, test = factorize_dfs(train, test, to_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictions using GroupCV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-19T18:59:57.923064Z",
     "start_time": "2020-03-19T18:59:57.912093Z"
    }
   },
   "outputs": [],
   "source": [
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-19T17:02:55.151943Z",
     "start_time": "2020-03-19T17:02:55.055888Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dates = pd.read_pickle('./data/train_dates.pkl')\n",
    "train_dates['Month'] = train_dates['TransactionDT'].dt.month\n",
    "# June has few observations, so we merge it with may\n",
    "train_dates.loc[train_dates['Month']==6, 'Month'] = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-19T17:02:59.277216Z",
     "start_time": "2020-03-19T17:02:59.274218Z"
    }
   },
   "outputs": [],
   "source": [
    "gkf = GroupKFold(n_splits=6)\n",
    "\n",
    "oof = np.zeros(len(train))\n",
    "preds = np.zeros(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-19T21:38:09.057745Z",
     "start_time": "2020-03-19T20:03:07.228772Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 withholding month 3\n",
      " rows of train = 488572 rows of holdout = 101968\n",
      "[0]\tvalidation_0-auc:0.84176\n",
      "Will train until validation_0-auc hasn't improved in 100 rounds.\n",
      "[100]\tvalidation_0-auc:0.92239\n",
      "[200]\tvalidation_0-auc:0.94588\n",
      "[300]\tvalidation_0-auc:0.95519\n",
      "[400]\tvalidation_0-auc:0.95765\n",
      "[500]\tvalidation_0-auc:0.95842\n",
      "[600]\tvalidation_0-auc:0.95847\n",
      "Stopping. Best iteration:\n",
      "[566]\tvalidation_0-auc:0.95860\n",
      "\n",
      "Fold 2 withholding month 1\n",
      " rows of train = 498030 rows of holdout = 92510\n",
      "[0]\tvalidation_0-auc:0.83524\n",
      "Will train until validation_0-auc hasn't improved in 100 rounds.\n",
      "[100]\tvalidation_0-auc:0.92307\n",
      "[200]\tvalidation_0-auc:0.94691\n",
      "[300]\tvalidation_0-auc:0.95888\n",
      "[400]\tvalidation_0-auc:0.96333\n",
      "[500]\tvalidation_0-auc:0.96485\n",
      "[600]\tvalidation_0-auc:0.96556\n",
      "[700]\tvalidation_0-auc:0.96603\n",
      "[800]\tvalidation_0-auc:0.96628\n",
      "Stopping. Best iteration:\n",
      "[779]\tvalidation_0-auc:0.96634\n",
      "\n",
      "Fold 3 withholding month 5\n",
      " rows of train = 498113 rows of holdout = 92427\n",
      "[0]\tvalidation_0-auc:0.84177\n",
      "Will train until validation_0-auc hasn't improved in 100 rounds.\n",
      "[100]\tvalidation_0-auc:0.91309\n",
      "[200]\tvalidation_0-auc:0.94009\n",
      "[300]\tvalidation_0-auc:0.94968\n",
      "[400]\tvalidation_0-auc:0.95305\n",
      "[500]\tvalidation_0-auc:0.95341\n",
      "Stopping. Best iteration:\n",
      "[481]\tvalidation_0-auc:0.95360\n",
      "\n",
      "Fold 4 withholding month 2\n",
      " rows of train = 504815 rows of holdout = 85725\n",
      "[0]\tvalidation_0-auc:0.84750\n",
      "Will train until validation_0-auc hasn't improved in 100 rounds.\n",
      "[100]\tvalidation_0-auc:0.94061\n",
      "[200]\tvalidation_0-auc:0.95749\n",
      "[300]\tvalidation_0-auc:0.96381\n",
      "[400]\tvalidation_0-auc:0.96567\n",
      "[500]\tvalidation_0-auc:0.96608\n",
      "[600]\tvalidation_0-auc:0.96603\n",
      "Stopping. Best iteration:\n",
      "[592]\tvalidation_0-auc:0.96617\n",
      "\n",
      "Fold 5 withholding month 4\n",
      " rows of train = 506969 rows of holdout = 83571\n",
      "[0]\tvalidation_0-auc:0.82950\n",
      "Will train until validation_0-auc hasn't improved in 100 rounds.\n",
      "[100]\tvalidation_0-auc:0.92373\n",
      "[200]\tvalidation_0-auc:0.95261\n",
      "[300]\tvalidation_0-auc:0.96395\n",
      "[400]\tvalidation_0-auc:0.96820\n",
      "[500]\tvalidation_0-auc:0.96955\n",
      "[600]\tvalidation_0-auc:0.97040\n",
      "[700]\tvalidation_0-auc:0.97076\n",
      "[800]\tvalidation_0-auc:0.97083\n",
      "Stopping. Best iteration:\n",
      "[725]\tvalidation_0-auc:0.97098\n",
      "\n",
      "####################\n",
      "XGB95 OOF CV= 0.9533002338917641\n"
     ]
    }
   ],
   "source": [
    "for i, (idxT, idxV) in enumerate(gkf.split(train_dates, y_train, groups=train_dates['Month'])):\n",
    "    \n",
    "    if i == 0: continue\n",
    "    \n",
    "    month = train_dates.iloc[idxV]['Month'].iloc[0]\n",
    "    print('Fold',i,'withholding month', month)\n",
    "    print(' rows of train =',len(idxT),'rows of holdout =',len(idxV))\n",
    "    \n",
    "    clf = xgb.XGBClassifier(\n",
    "            n_estimators=2000,\n",
    "            max_depth=12,\n",
    "            learning_rate=0.02,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.4,\n",
    "            missing=-1,\n",
    "            eval_metric='auc',\n",
    "            # USE CPU\n",
    "            nthread=4,\n",
    "            tree_method='hist'\n",
    "        )     \n",
    "    \n",
    "    h = clf.fit(train.iloc[idxT], y_train.iloc[idxT],\n",
    "                eval_set=[(train.iloc[idxV], y_train.iloc[idxV])],\n",
    "                verbose=100, early_stopping_rounds=100)\n",
    "    \n",
    "    oof[idxV] += clf.predict_proba(train.iloc[idxV])[:,1]\n",
    "    preds += clf.predict_proba(test)[:,1]/gkf.n_splits\n",
    "    del h, clf\n",
    "    gc.collect()\n",
    "    \n",
    "print('#'*20)\n",
    "print ('XGB95 OOF CV=',roc_auc_score(y_train,oof))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-19T21:47:03.521415Z",
     "start_time": "2020-03-19T21:47:03.402317Z"
    }
   },
   "outputs": [],
   "source": [
    "sample = pd.read_csv('./data/sample_submission.csv')\n",
    "submission = sample\n",
    "submission['isFraud'] = preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-19T21:49:25.224074Z",
     "start_time": "2020-03-19T21:49:23.633115Z"
    }
   },
   "outputs": [],
   "source": [
    "submission.to_csv('./data/submissions/xbg_6kfold.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dl]",
   "language": "python",
   "name": "conda-env-dl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
