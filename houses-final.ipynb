{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T23:05:54.492389Z",
     "start_time": "2020-03-31T23:05:54.485384Z"
    }
   },
   "outputs": [],
   "source": [
    "# Standard Packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import math\n",
    "import pdb\n",
    "from collections import defaultdict\n",
    "\n",
    "# Pandas API\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "from pandas.api.types import is_string_dtype\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, ClassifierMixin\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LassoCV, LinearRegression, RidgeCV\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# scipy/stats\n",
    "from scipy.stats import skew\n",
    "from scipy.special import boxcox1p\n",
    "from scipy.stats import boxcox_normmax\n",
    "\n",
    "\n",
    "# XGBoost / LightGBM\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import catboost\n",
    "\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "from pandas.api.types import is_string_dtype\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Classes and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T17:23:42.192067Z",
     "start_time": "2020-03-31T17:23:42.182186Z"
    }
   },
   "outputs": [],
   "source": [
    "class FixMissing(BaseEstimator, TransformerMixin):\n",
    "    \"\"\" Imputes meadian value for missing values\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    na_column : boolean, optional (default = 1)\n",
    "        If true, will add columns keeping track of\n",
    "        missing values on the original data\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, na_column=True):\n",
    "        self.na_column = na_column\n",
    "    \n",
    "    def fit(self, df, y=None):\n",
    "\n",
    "        # filler data for columns with NA values\n",
    "        self.na_dict = defaultdict(int)\n",
    "        # boolean for columns with NA values\n",
    "        self.is_na_column = []\n",
    "        \n",
    "        # filling dictionaries\n",
    "        for name, column in df.items():\n",
    "            if pd.isnull(column).sum():\n",
    "                self.is_na_column.append(name)\n",
    "            if(is_numeric_dtype(column)):\n",
    "                filler = column.median()\n",
    "                self.na_dict[name] = filler\n",
    "        return self\n",
    "                \n",
    "    def transform(self, df):\n",
    "        self.df = df.copy()\n",
    "        for name, column in self.df.items():\n",
    "    \n",
    "            if name in self.is_na_column and self.na_column:\n",
    "                self.df[name + '_na'] = pd.isnull(column)\n",
    "                \n",
    "            if (is_numeric_dtype(column) and pd.isnull(column).sum()):\n",
    "                self.df.loc[:, name] = column.fillna(self.na_dict[name])\n",
    "            # corner case where all values on the fit step are nan\n",
    "            # drop the column if that's the case\n",
    "            if math.isnan(self.na_dict[name]):\n",
    "                self.df.drop(columns=name, inplace=True)\n",
    "        return self.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T17:23:42.211626Z",
     "start_time": "2020-03-31T17:23:42.194099Z"
    }
   },
   "outputs": [],
   "source": [
    "class MakeCategorical(BaseEstimator, TransformerMixin):\n",
    "    \"\"\" Label encoding for columns with string/categorical data\n",
    "        NA values are transformed into 0\n",
    "        \n",
    "    Parameters\n",
    "    ----------\n",
    "    skip : list of strings, optional (default = [])\n",
    "        skip these columns\n",
    "    fix_na : boolean, optional (default = 1)\n",
    "        If true, NA values will be changed to zero\n",
    "    max_cat : integer (default = 0)\n",
    "        maximum number of categories for one-hot encoding\n",
    "    \"\"\"    \n",
    "    \n",
    "    def __init__(self, max_cat, skip = [], fix_na = True):\n",
    "        self.skip = skip\n",
    "        self.fix_na = fix_na\n",
    "        self.max_cat = max_cat\n",
    "    \n",
    "    def fit(self, df, y=None):\n",
    "        \n",
    "        self.cat_dict = {}\n",
    "        self.onehot_list = []\n",
    "        \n",
    "        for name, col in df.items():\n",
    "            if is_string_dtype(col):\n",
    "                # dictionary with categories from the training set\n",
    "                self.cat_dict.update({name:col.astype('category').cat.categories})\n",
    "                # list with columns for one hot encoding\n",
    "                if len(self.cat_dict[name]) <= self.max_cat:\n",
    "                    self.onehot_list.append(name)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, df):\n",
    "        \n",
    "        self.df = df.copy()\n",
    "        for name, col in df.items():\n",
    "            if is_string_dtype(col) and name not in self.skip:\n",
    "                \n",
    "                # convert to categories on the training set\n",
    "                temp = col.astype('category')\n",
    "                self.df[name] = temp.cat.set_categories(self.cat_dict[name])\n",
    "                \n",
    "                # one hot-encoding for selected columns\n",
    "                if name in self.onehot_list:\n",
    "                    sub_df = pd.get_dummies(self.df[name])\n",
    "                    sub_df.rename(columns = {n:name+'_'+n for n in sub_df.columns}, inplace=True)\n",
    "                    self.df = pd.concat((self.df, sub_df), axis=1)\n",
    "                    self.df.drop(columns=name, inplace=True)\n",
    "                # change to numerical values if not one hot encoded\n",
    "                else:\n",
    "                    self.df[name] = temp.cat.set_categories(self.cat_dict[name]).cat.codes + 1\n",
    "                                    \n",
    "                if not self.fix_na:\n",
    "                    self.df.loc[self.df[name]==0, name] = np.nan\n",
    "                    \n",
    "        return self.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T17:23:42.219626Z",
     "start_time": "2020-03-31T17:23:42.213625Z"
    }
   },
   "outputs": [],
   "source": [
    "class TargetMedianEncoding(BaseEstimator, TransformerMixin):\n",
    "    \"\"\" TO DO\n",
    "    \"\"\"    \n",
    "    \n",
    "    def __init__(self, column):\n",
    "        \n",
    "        self.column = column\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        df = pd.concat([X[self.column], y], axis=1)\n",
    "        self.map = df.groupby(self.column).median().iloc[:, 0]\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \n",
    "        self.df = X.copy()\n",
    "        self.df[self.column+'_median'] = X[self.column].map(self.map)\n",
    "        \n",
    "        return self.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T17:23:42.226648Z",
     "start_time": "2020-03-31T17:23:42.221628Z"
    }
   },
   "outputs": [],
   "source": [
    "def clip_data(data, columns, quantiles=[.01, .99]):\n",
    "    \n",
    "    # lower and upper limits for all variables\n",
    "    lower_dict = {column:np.quantile(data[column], quantiles[0]) for column in columns}\n",
    "    upper_dict = {column:np.quantile(data[column], quantiles[1]) for column in columns}\n",
    "    \n",
    "    # clips dataset\n",
    "    for column in columns:\n",
    "        data = data[(data[column] >= lower_dict[column]) & (data[column] <= upper_dict[column])]\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T17:23:42.235490Z",
     "start_time": "2020-03-31T17:23:42.228669Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_skewness(data):\n",
    "    \n",
    "    numerical_variables = [column for column in data.columns if is_numeric_dtype(data[column])]\n",
    "    skewness = [skew(data[col]) for col in numerical_variables]\n",
    "    \n",
    "    df = pd.DataFrame(dict(variable=numerical_variables,\n",
    "                  skewness=skewness)).sort_values('skewness', ascending=False)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T17:25:04.642780Z",
     "start_time": "2020-03-31T17:25:04.635785Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data_description.txt',\n",
       " 'parsed_xgb',\n",
       " 'sample_submission.csv',\n",
       " 'test.csv',\n",
       " 'train - Copy.csv',\n",
       " 'train.csv']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_FOLDER = './data/house_prices/'\n",
    "os.listdir(DATA_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T17:25:04.707575Z",
     "start_time": "2020-03-31T17:25:04.645400Z"
    }
   },
   "outputs": [],
   "source": [
    "sample = pd.read_csv(os.path.join(DATA_FOLDER, 'sample_submission.csv'))\n",
    "train = pd.read_csv(os.path.join(DATA_FOLDER, 'train.csv'))\n",
    "test = pd.read_csv(os.path.join(DATA_FOLDER, 'test.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adjusting Outliers - Huge Impact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We clip the datasets based on the given quantiles, using the columns listes. Important: we calculate the outliers for every columns before clipping (not in a sequentially manner)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T17:25:13.354061Z",
     "start_time": "2020-03-31T17:25:13.332059Z"
    }
   },
   "outputs": [],
   "source": [
    "columns = ['GrLivArea', 'LotArea', 'TotalBsmtSF', '1stFlrSF', 'BsmtFinSF1']\n",
    "train = clip_data(train, columns, quantiles=[0.01, 0.99])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenating train and test data and transforming SalePrice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T17:25:13.378773Z",
     "start_time": "2020-03-31T17:25:13.355740Z"
    }
   },
   "outputs": [],
   "source": [
    "y = np.log(train['SalePrice'])\n",
    "data = pd.concat([train.drop(columns='SalePrice'), test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping Id:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T17:25:13.391774Z",
     "start_time": "2020-03-31T17:25:13.381776Z"
    }
   },
   "outputs": [],
   "source": [
    "data.drop(columns='Id', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fixing Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using None for categorical variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T17:25:26.105857Z",
     "start_time": "2020-03-31T17:25:26.101236Z"
    }
   },
   "outputs": [],
   "source": [
    "fix_na_none = ['MiscFeature', 'Alley', 'Fence', 'FireplaceQu',\n",
    "               'GarageType', 'GarageFinish', 'GarageQual',\n",
    "               'GarageCond', 'GarageYrBlt', 'BsmtQual',\n",
    "               'BsmtCond', 'BsmtExposure', 'BsmtFinType1',\n",
    "               'BsmtFinType2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T17:25:26.122900Z",
     "start_time": "2020-03-31T17:25:26.108854Z"
    }
   },
   "outputs": [],
   "source": [
    "for col in fix_na_none:\n",
    "    data[col].fillna('None', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add a boolean for pool and remove the original columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T17:25:26.137458Z",
     "start_time": "2020-03-31T17:25:26.124462Z"
    }
   },
   "outputs": [],
   "source": [
    "data['has_pool'] = np.where(data['PoolQC'].isna(), 0, 1)\n",
    "data.drop(columns=['PoolQC', 'PoolArea'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the median value of LotFrontage for missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T17:25:26.158537Z",
     "start_time": "2020-03-31T17:25:26.139467Z"
    }
   },
   "outputs": [],
   "source": [
    "data['LotFrontage'] = data.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will deal with the remaining missing values on the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some features suggested on popular kaggle kernels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T17:25:30.230319Z",
     "start_time": "2020-03-31T17:25:30.203132Z"
    }
   },
   "outputs": [],
   "source": [
    "data['YrBltAndRemod']=data['YearBuilt']+data['YearRemodAdd']\n",
    "data['TotalSF']=data['TotalBsmtSF'] + data['1stFlrSF'] + data['2ndFlrSF']\n",
    "\n",
    "data['Total_sqr_footage'] = (data['BsmtFinSF1'] + data['BsmtFinSF2'] +\n",
    "                                 data['1stFlrSF'] + data['2ndFlrSF'])\n",
    "\n",
    "data['Total_Bathrooms'] = (data['FullBath'] + (0.5 * data['HalfBath']) +\n",
    "                               data['BsmtFullBath'] + (0.5 * data['BsmtHalfBath']))\n",
    "\n",
    "data['Total_porch_sf'] = (data['OpenPorchSF'] + data['3SsnPorch'] +\n",
    "                              data['EnclosedPorch'] + data['ScreenPorch'] +\n",
    "                              data['WoodDeckSF'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some other features and interactions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T17:25:37.172647Z",
     "start_time": "2020-03-31T17:25:37.156635Z"
    }
   },
   "outputs": [],
   "source": [
    "data['TotalSF_OverallQual'] = data['TotalSF']*data['OverallQual']\n",
    "\n",
    "good_bsm = np.where(data['BsmtFinType1']=='GLQ', 1, 0)\n",
    "avg_bsm = np.where(data['BsmtFinType1']=='ALQ', 1, 0)\n",
    "\n",
    "data['good_TotalSF'] = data['BsmtFinSF1']*good_bsm+data['1stFlrSF']+data['2ndFlrSF']\n",
    "data['avg_TotalSF'] = data['BsmtFinSF1']*avg_bsm+data['1stFlrSF']+data['2ndFlrSF']\n",
    "\n",
    "\n",
    "data['SaleCondition_SaleType'] = data['SaleCondition'].astype(str)+data['SaleType'].astype(str)\n",
    "\n",
    "\n",
    "data['bin_TotalSF'] = pd.cut(data['TotalSF'],bins=10, labels=False)+1 \n",
    "\n",
    "data['OverallQual*TotalBsmtSF'] = data['OverallQual']*data['TotalBsmtSF']\n",
    "\n",
    "\n",
    "data['GarageCars*OverallQual'] = data['OverallQual']*data['GarageCars']\n",
    "\n",
    "\n",
    "data['OverallQual*TotalBsmtSF'] = data['OverallQual']*data['TotalBsmtSF']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few more..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T17:25:42.241603Z",
     "start_time": "2020-03-31T17:25:42.233663Z"
    }
   },
   "outputs": [],
   "source": [
    "data['TotalSF**2'] = data['TotalSF']**2\n",
    "data['good_TotalSF**2'] = data['good_TotalSF']**2\n",
    "data['OverallQual**2'] = data['OverallQual']**2\n",
    "data['avg_TotalSF**2'] = data['good_TotalSF']**2\n",
    "data['LotArea**2'] = data['LotArea']**2\n",
    "data['OverallCond**2'] = data['OverallCond']**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adjusting Skewness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gettin a list of skewed variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T17:25:44.057026Z",
     "start_time": "2020-03-31T17:25:44.024236Z"
    }
   },
   "outputs": [],
   "source": [
    "numerical_variables = [column for column in data.columns if is_numeric_dtype(data[column])]\n",
    "skew_df = calculate_skewness(data)\n",
    "skewed_columns = skew_df[skew_df['skewness'] > 0.5]['variable'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Box Cox Transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T17:25:45.438551Z",
     "start_time": "2020-03-31T17:25:45.252654Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USUARIO\\Anaconda3\\envs\\dl\\lib\\site-packages\\scipy\\stats\\stats.py:3508: PearsonRConstantInputWarning: An input array is constant; the correlation coefficent is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    }
   ],
   "source": [
    "for col in skewed_columns:\n",
    "    data[col] = boxcox1p(data[col], boxcox_normmax(data[col] + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropping Some Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T17:25:53.320427Z",
     "start_time": "2020-03-31T17:25:53.300472Z"
    }
   },
   "outputs": [],
   "source": [
    "data.drop(columns=['BsmtFinType2', 'RoofMatl', 'LandContour',\n",
    "                   'BsmtFinType1', 'MasVnrType', 'LowQualFinSF',\n",
    "                   'Heating', 'HouseStyle', 'BsmtFinSF2',\n",
    "                   'BedroomAbvGr', 'PavedDrive', 'Alley',\n",
    "                   'Exterior2nd', 'LotConfig',\n",
    "                   'TotalBsmtSF', 'RoofStyle', 'YrSold',\n",
    "                   'SaleType', 'ScreenPorch', 'FullBath',\n",
    "                   'MSSubClass', 'MasVnrArea', 'LotFrontage',\n",
    "                   'BsmtCond', 'WoodDeckSF', 'LotShape',\n",
    "                   'MoSold'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going Back to Train and Test Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T17:25:56.667210Z",
     "start_time": "2020-03-31T17:25:56.660170Z"
    }
   },
   "outputs": [],
   "source": [
    "train = data[:len(y)]\n",
    "test = data[len(y):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build our baseline pipeline and evaluate its performance using cross-validation. The steps on the pipeline are:  \n",
    "1. Median Encoding the Target values based on Neighborhood;\n",
    "2. Fix remaining missing values;\n",
    "3. Transforming categorical variables (one-hot encoding for categories with less than max_cat features and label encoding for the rest);\n",
    "4. Scalling the data;\n",
    "5. LassoCV using prespecified alpha values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T17:26:49.356201Z",
     "start_time": "2020-03-31T17:26:49.351201Z"
    }
   },
   "outputs": [],
   "source": [
    "alphas = [5e-05, 0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008]\n",
    "scaler = RobustScaler()\n",
    "\n",
    "lassocv_pipeline = Pipeline([\n",
    "    ('encoder1', TargetMedianEncoding(column='Neighborhood')),\n",
    "    ('fix_missing', FixMissing(na_column=False)),\n",
    "    ('make_categorical', MakeCategorical(max_cat=60)),\n",
    "    ('scaler', scaler),\n",
    "    ('lasso', LassoCV(max_iter=1e7, alphas=alphas, \n",
    "                      random_state=42, cv=5))\n",
    "])\n",
    "\n",
    "kfolds = KFold(n_splits=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that all the steps on the pipeline above are done respecting train-validation split methodology (taht is, no leakage). Let's evaluate the whole pipeline using cross-validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T17:27:00.515243Z",
     "start_time": "2020-03-31T17:26:50.789379Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10744128214262771"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.sqrt(-cross_val_score(lassocv_pipeline, train, y, scoring=\"neg_mean_squared_error\",\n",
    "                                 cv=kfolds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model alone achieves a pretty good score on the Public Leaderbord: 12747.97 (around top 2.7% percentile). Obs: we are evaluating the models using log(y) on this notebook, while the Public Leaderboard shows results using the original scale. Since the transformation is monotonic, we can use log when doing local validaiton. Preparing the submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lassocv_pipeline.fit(train, y);\n",
    "y_pred = lassocv_pipeline.predict(test)\n",
    "sample['SalePrice'] = np.exp(y_pred)\n",
    "sample.to_csv('./data/predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's tune an XGBoost model and see if we can improve on the baseline model. We will preprocess the dataset before tuning the model, so we can iterate faster. After selecting a good set of parameters, we can evaluate the pipeline like we did above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T17:47:42.855083Z",
     "start_time": "2020-03-31T17:47:42.851080Z"
    }
   },
   "outputs": [],
   "source": [
    "baseline_xgb = xgb.XGBRegressor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T17:46:59.699234Z",
     "start_time": "2020-03-31T17:46:59.693204Z"
    }
   },
   "outputs": [],
   "source": [
    "scaler = RobustScaler()\n",
    "\n",
    "preprocess_pipeline = Pipeline([\n",
    "    ('encoder1', TargetMedianEncoding(column='Neighborhood')),\n",
    "    ('fix_missing', FixMissing(na_column=False)),\n",
    "    ('make_categorical', MakeCategorical(max_cat=60)),\n",
    "    ('scaler', scaler)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T17:47:00.000287Z",
     "start_time": "2020-03-31T17:46:59.701202Z"
    }
   },
   "outputs": [],
   "source": [
    "train_proc = preprocess_pipeline.fit_transform(train, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating the performance using XGBoost with the default parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T17:48:16.922345Z",
     "start_time": "2020-03-31T17:48:10.814889Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12833051853824423"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.sqrt(-cross_val_score(baseline_xgb, \n",
    "                                 train_proc, y, \n",
    "                                 scoring=\"neg_mean_squared_error\",\n",
    "                                 cv=10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find the initial number of estimators for some standard parameters configurations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T17:53:26.452798Z",
     "start_time": "2020-03-31T17:53:26.445811Z"
    }
   },
   "outputs": [],
   "source": [
    "def find_nrounds_xgb(model, X, y, metrics='rmse',\n",
    "                     cv_folds=5, early_stopping_rounds=50):\n",
    "    \n",
    "    xgtrain = xgb.DMatrix(X, y)\n",
    "    \n",
    "    params = model.get_xgb_params()\n",
    "    \n",
    "    cvresult = xgb.cv(params, xgtrain, metrics=metrics, \n",
    "                      num_boost_round=params['n_estimators'],\n",
    "                      early_stopping_rounds=early_stopping_rounds)\n",
    "    \n",
    "    # Setting optimal number of estimators\n",
    "    n_rounds_optimal = cvresult.shape[0]\n",
    "    model.set_params(n_estimators=n_rounds_optimal)\n",
    "    \n",
    "    print(cvresult.iloc[-1, :])\n",
    "    print(f\"n_estimators:{n_rounds_optimal}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T17:54:49.576771Z",
     "start_time": "2020-03-31T17:54:49.568772Z"
    }
   },
   "outputs": [],
   "source": [
    "def grid_search(estimator, X, y, params, scoring, cv=4, random=True,\n",
    "                n_iter=150, n_jobs=6):\n",
    "    \n",
    "    if random:\n",
    "        random_search = RandomizedSearchCV(estimator, \n",
    "                                           param_distributions=params,\n",
    "                                           n_iter=n_iter, n_jobs=n_jobs, \n",
    "                                           cv=cv, scoring=scoring,\n",
    "                                           verbose=3, random_state=340)\n",
    "    \n",
    "    else:\n",
    "        random_search = GridSearchCV(estimator, param_grid=params, \n",
    "                                       n_jobs=n_jobs, cv=cv,\n",
    "                                       scoring='neg_mean_absolute_error',\n",
    "                                       verbose=3)\n",
    "\n",
    "    random_search.fit(X, y)\n",
    "    \n",
    "    return random_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T17:53:27.877531Z",
     "start_time": "2020-03-31T17:53:27.871399Z"
    }
   },
   "outputs": [],
   "source": [
    "model = xgb.XGBRegressor(learning_rate=0.1, n_estimators=1000,\n",
    "                         max_depth=5, min_child_weight=1,\n",
    "                         gamma=0, colsample_bytree=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T17:53:30.795966Z",
     "start_time": "2020-03-31T17:53:27.880391Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-rmse-mean    0.026588\n",
      "train-rmse-std     0.001133\n",
      "test-rmse-mean     0.120527\n",
      "test-rmse-std      0.001983\n",
      "Name: 186, dtype: float64\n",
      "n_estimators:187\n"
     ]
    }
   ],
   "source": [
    "model = find_nrounds_xgb(model, train_proc, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **max_depth** / **min_child_weight**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T18:00:42.529891Z",
     "start_time": "2020-03-31T18:00:42.525861Z"
    }
   },
   "outputs": [],
   "source": [
    "param_test1 = {\n",
    " 'max_depth':range(3,10,2),\n",
    " 'min_child_weight':range(1,6,2)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T18:01:21.869038Z",
     "start_time": "2020-03-31T18:00:42.531858Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 12 candidates, totalling 48 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  20 tasks      | elapsed:   12.1s\n",
      "[Parallel(n_jobs=6)]: Done  48 out of  48 | elapsed:   38.4s finished\n"
     ]
    }
   ],
   "source": [
    "results = grid_search(model, train_proc, y, param_test1, \n",
    "                      scoring='neg_mean_squared_error',\n",
    "                      random=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T18:01:21.882039Z",
     "start_time": "2020-03-31T18:01:21.874037Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.07867467679097549\n",
      "{'max_depth': 3, 'min_child_weight': 5}\n"
     ]
    }
   ],
   "source": [
    "print(f\"MSE: {-results.best_score_}\")\n",
    "print(results.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We hit the lower bound for maximum depth, let's add a few more options and try again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T18:01:30.275719Z",
     "start_time": "2020-03-31T18:01:30.272759Z"
    }
   },
   "outputs": [],
   "source": [
    "param_test2 = {\n",
    " 'max_depth':range(1,5,1),\n",
    " 'min_child_weight':range(1,6,2)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T18:01:55.269458Z",
     "start_time": "2020-03-31T18:01:30.277751Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 12 candidates, totalling 48 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  20 tasks      | elapsed:    7.4s\n",
      "[Parallel(n_jobs=6)]: Done  48 out of  48 | elapsed:   23.8s finished\n"
     ]
    }
   ],
   "source": [
    "results = grid_search(model, train_proc, y, param_test2, \n",
    "                      scoring='neg_mean_squared_error',\n",
    "                      random=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we ended inside the range of parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T18:01:55.279464Z",
     "start_time": "2020-03-31T18:01:55.274462Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.07791161190825713\n",
      "{'max_depth': 4, 'min_child_weight': 5}\n"
     ]
    }
   ],
   "source": [
    "print(f\"MSE: {-results.best_score_}\")\n",
    "print(results.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Updating out model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T18:02:06.347172Z",
     "start_time": "2020-03-31T18:02:06.343172Z"
    }
   },
   "outputs": [],
   "source": [
    "model.set_params(**results.best_params_);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **gamma**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T18:02:09.325947Z",
     "start_time": "2020-03-31T18:02:09.321947Z"
    }
   },
   "outputs": [],
   "source": [
    "param_test3 = {\n",
    " 'gamma':[i/10.0 for i in range(0,51)]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T18:04:35.955581Z",
     "start_time": "2020-03-31T18:02:12.080306Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 51 candidates, totalling 204 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  20 tasks      | elapsed:   14.4s\n",
      "[Parallel(n_jobs=6)]: Done 116 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=6)]: Done 204 out of 204 | elapsed:  2.4min finished\n"
     ]
    }
   ],
   "source": [
    "results = grid_search(model, train_proc, y, param_test3, \n",
    "                      scoring='neg_mean_squared_error',\n",
    "                      random=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gamma equals 0 (minimum) is the best values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T18:06:58.891747Z",
     "start_time": "2020-03-31T18:06:58.886744Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.07791161190825713\n",
      "{'gamma': 0.0}\n"
     ]
    }
   ],
   "source": [
    "print(f\"MSE: {-results.best_score_}\")\n",
    "print(results.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T18:07:27.400916Z",
     "start_time": "2020-03-31T18:07:27.394915Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=None, booster=None, colsample_bylevel=None,\n",
       "             colsample_bynode=None, colsample_bytree=0.8, gamma=0.0,\n",
       "             gpu_id=None, importance_type='gain', interaction_constraints=None,\n",
       "             learning_rate=0.1, max_delta_step=None, max_depth=4,\n",
       "             min_child_weight=5, missing=nan, monotone_constraints=None,\n",
       "             n_estimators=187, n_jobs=None, num_parallel_tree=None,\n",
       "             objective='reg:squarederror', random_state=None, reg_alpha=None,\n",
       "             reg_lambda=None, scale_pos_weight=None, subsample=None,\n",
       "             tree_method=None, validate_parameters=False, verbosity=None)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.set_params(**results.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **subsample / cosample_bytree**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T18:07:55.246772Z",
     "start_time": "2020-03-31T18:07:55.241757Z"
    }
   },
   "outputs": [],
   "source": [
    "param_test4 = {\n",
    " 'subsample':[i/10.0 for i in range(6,10)],\n",
    " 'colsample_bytree':[i/10.0 for i in range(6,10)]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T18:08:32.783593Z",
     "start_time": "2020-03-31T18:07:55.248777Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 16 candidates, totalling 64 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  20 tasks      | elapsed:   10.3s\n",
      "[Parallel(n_jobs=6)]: Done  64 out of  64 | elapsed:   36.6s finished\n"
     ]
    }
   ],
   "source": [
    "results = grid_search(model, train_proc, y, param_test4, \n",
    "                      scoring='neg_mean_squared_error',\n",
    "                      random=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T18:08:39.208839Z",
     "start_time": "2020-03-31T18:08:39.204839Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.07722327081465681\n",
      "{'colsample_bytree': 0.6, 'subsample': 0.8}\n"
     ]
    }
   ],
   "source": [
    "print(f\"MSE: {-results.best_score_}\")\n",
    "print(results.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's expand the bounds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T18:12:17.279574Z",
     "start_time": "2020-03-31T18:12:17.274570Z"
    }
   },
   "outputs": [],
   "source": [
    "param_test5 = {\n",
    " 'subsample':[i/10.0 for i in range(6,10)],\n",
    " 'colsample_bytree':[i/10.0 for i in range(1,10)]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T18:13:27.471489Z",
     "start_time": "2020-03-31T18:12:20.591918Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 36 candidates, totalling 144 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  20 tasks      | elapsed:    4.8s\n",
      "[Parallel(n_jobs=6)]: Done 116 tasks      | elapsed:   47.6s\n",
      "[Parallel(n_jobs=6)]: Done 144 out of 144 | elapsed:  1.1min finished\n"
     ]
    }
   ],
   "source": [
    "results = grid_search(model, train_proc, y, param_test5, \n",
    "                      scoring='neg_mean_squared_error',\n",
    "                      random=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T18:13:27.481488Z",
     "start_time": "2020-03-31T18:13:27.475487Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.0759563339131957\n",
      "{'colsample_bytree': 0.2, 'subsample': 0.9}\n"
     ]
    }
   ],
   "source": [
    "print(f\"MSE: {-results.best_score_}\")\n",
    "print(results.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T18:13:39.408895Z",
     "start_time": "2020-03-31T18:13:39.400906Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=None, booster=None, colsample_bylevel=None,\n",
       "             colsample_bynode=None, colsample_bytree=0.2, gamma=0.0,\n",
       "             gpu_id=None, importance_type='gain', interaction_constraints=None,\n",
       "             learning_rate=0.1, max_delta_step=None, max_depth=4,\n",
       "             min_child_weight=5, missing=nan, monotone_constraints=None,\n",
       "             n_estimators=187, n_jobs=None, num_parallel_tree=None,\n",
       "             objective='reg:squarederror', random_state=None, reg_alpha=None,\n",
       "             reg_lambda=None, scale_pos_weight=None, subsample=0.9,\n",
       "             tree_method=None, validate_parameters=False, verbosity=None)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.set_params(**results.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's reduce the learning rate and increase the number of estimators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T18:13:46.748928Z",
     "start_time": "2020-03-31T18:13:46.743278Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=None, booster=None, colsample_bylevel=None,\n",
       "             colsample_bynode=None, colsample_bytree=0.2, gamma=0.0,\n",
       "             gpu_id=None, importance_type='gain', interaction_constraints=None,\n",
       "             learning_rate=0.01, max_delta_step=None, max_depth=4,\n",
       "             min_child_weight=5, missing=nan, monotone_constraints=None,\n",
       "             n_estimators=5000, n_jobs=None, num_parallel_tree=None,\n",
       "             objective='reg:squarederror', random_state=None, reg_alpha=None,\n",
       "             reg_lambda=None, scale_pos_weight=None, subsample=0.9,\n",
       "             tree_method=None, validate_parameters=False, verbosity=None)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.set_params(**{'learning_rate':0.01, 'n_estimators':5000})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final Performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T18:14:31.805382Z",
     "start_time": "2020-03-31T18:14:31.800387Z"
    }
   },
   "outputs": [],
   "source": [
    "scaler = RobustScaler()\n",
    "\n",
    "xgbcv_pipeline = Pipeline([\n",
    "    ('encoder1', TargetMedianEncoding(column='Neighborhood')),\n",
    "    ('fix_missing', FixMissing(na_column=False)),\n",
    "    ('make_categorical', MakeCategorical(max_cat=60)),\n",
    "    ('scaler', scaler),\n",
    "    ('xgb', model)\n",
    "])\n",
    "\n",
    "kfolds = KFold(n_splits=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slightly worse than the LassoCV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T18:17:39.356799Z",
     "start_time": "2020-03-31T18:15:33.026042Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10892592669298158"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.sqrt(-cross_val_score(xgbcv_pipeline, train, y, scoring=\"neg_mean_squared_error\",\n",
    "                                 cv=kfolds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing the Submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T18:18:43.127242Z",
     "start_time": "2020-03-31T18:18:30.047756Z"
    }
   },
   "outputs": [],
   "source": [
    "xgbcv_pipeline.fit(train, y);\n",
    "y_pred = xgbcv_pipeline.predict(test)\n",
    "sample['SalePrice'] = np.exp(y_pred)\n",
    "sample.to_csv('./data/predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading baseline models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T19:52:58.491842Z",
     "start_time": "2020-03-31T19:52:58.485826Z"
    }
   },
   "outputs": [],
   "source": [
    "alphas = [5e-05, 0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008]\n",
    "scaler = RobustScaler()\n",
    "\n",
    "lassocv_pipeline = Pipeline([\n",
    "    ('encoder1', TargetMedianEncoding(column='Neighborhood')),\n",
    "    ('fix_missing', FixMissing(na_column=False)),\n",
    "    ('make_categorical', MakeCategorical(max_cat=60)),\n",
    "    ('scaler', scaler),\n",
    "    ('lasso', LassoCV(max_iter=1e7, alphas=alphas, \n",
    "                      random_state=42, cv=5))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T19:52:59.312176Z",
     "start_time": "2020-03-31T19:52:59.305149Z"
    }
   },
   "outputs": [],
   "source": [
    "scaler = RobustScaler()\n",
    "\n",
    "xgb_pipeline = Pipeline([\n",
    "    ('encoder1', TargetMedianEncoding(column='Neighborhood')),\n",
    "    ('fix_missing', FixMissing(na_column=False)),\n",
    "    ('make_categorical', MakeCategorical(max_cat=60)),\n",
    "    ('scaler', scaler),\n",
    "    ('xgb', model)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a stacking estimator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T20:14:32.280606Z",
     "start_time": "2020-03-31T20:14:32.268608Z"
    }
   },
   "outputs": [],
   "source": [
    "class Stacking(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\" TO DO\n",
    "    \"\"\"    \n",
    "    \n",
    "    def __init__(self, stack_model, pipelines, cv_stack=5):\n",
    "        \n",
    "        self.stack_model = stack_model\n",
    "        self.pipelines = pipelines\n",
    "        self.cv_stack = cv_stack\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        cv = KFold(n_splits=self.cv_stack, \n",
    "                   shuffle=True, random_state=583)\n",
    "        \n",
    "        # out of fold matrix with dimension (len(y), number of models)\n",
    "        y_oof = np.zeros(shape=(len(y), len(self.pipelines))) # Validation\n",
    "        \n",
    "        # calculate the oof predictions of y for every model\n",
    "        for i, pipeline in enumerate(self.pipelines):\n",
    "            \n",
    "            for tr, tt in cv.split(X, y):\n",
    "                pipeline.fit(X.iloc[tr, :], y.iloc[tr])\n",
    "                y_oof[tt, i] = pipeline.predict(X.iloc[tt, :])\n",
    "        \n",
    "        # fit the stacking model, using the oof ys as features and y as target\n",
    "        self.y_oof = y_oof\n",
    "        self.stack_model.fit(y_oof, y)\n",
    "        \n",
    "        # refitting the pipelines (for the prediction stage)\n",
    "        for pipeline in self.pipelines:\n",
    "            pipeline.fit(X, y)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \n",
    "        # calculate the predictions for all pipelines\n",
    "        predictions = np.zeros(shape=(len(X), len(self.pipelines))) \n",
    "        for i, pipeline in enumerate(self.pipelines):\n",
    "            \n",
    "            predictions[:, i] = pipeline.predict(X)\n",
    "        \n",
    "        # combine these predictions using the stacking model\n",
    "        y_final = self.stack_model.predict(predictions)\n",
    "        \n",
    "        return y_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T20:14:36.574086Z",
     "start_time": "2020-03-31T20:14:36.570053Z"
    }
   },
   "outputs": [],
   "source": [
    "pipelines = [lassocv_pipeline, xgb_pipeline]\n",
    "stacker = Stacking(LinearRegression(), pipelines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T20:15:51.728311Z",
     "start_time": "2020-03-31T20:14:36.578055Z"
    }
   },
   "outputs": [],
   "source": [
    "results = stacker.fit(train, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting Stacking (1st level and 2nd level models):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T21:05:51.388370Z",
     "start_time": "2020-03-31T21:04:11.165602Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Stacking(cv_stack=5,\n",
       "         pipelines=[Pipeline(memory=None,\n",
       "                             steps=[('encoder1',\n",
       "                                     TargetMedianEncoding(column='Neighborhood')),\n",
       "                                    ('fix_missing',\n",
       "                                     FixMissing(na_column=False)),\n",
       "                                    ('make_categorical',\n",
       "                                     MakeCategorical(fix_na=True, max_cat=60,\n",
       "                                                     skip=[])),\n",
       "                                    ('scaler',\n",
       "                                     RobustScaler(copy=True,\n",
       "                                                  quantile_range=(25.0, 75.0),\n",
       "                                                  with_centering=True,\n",
       "                                                  with_scaling=True)),\n",
       "                                    ('lasso',\n",
       "                                     LassoCV(alph...\n",
       "                                                  missing=nan,\n",
       "                                                  monotone_constraints=None,\n",
       "                                                  n_estimators=5000, n_jobs=0,\n",
       "                                                  num_parallel_tree=1,\n",
       "                                                  objective='reg:squarederror',\n",
       "                                                  random_state=0, reg_alpha=0,\n",
       "                                                  reg_lambda=1,\n",
       "                                                  scale_pos_weight=1,\n",
       "                                                  subsample=0.9,\n",
       "                                                  tree_method=None,\n",
       "                                                  validate_parameters=False,\n",
       "                                                  verbosity=None))],\n",
       "                             verbose=False)],\n",
       "         stack_model=LinearRegression(copy_X=True, fit_intercept=True,\n",
       "                                      n_jobs=None, normalize=False))"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacker.fit(train, y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing Submissions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T21:06:14.688730Z",
     "start_time": "2020-03-31T21:06:14.136710Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = stacker.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T21:06:17.139138Z",
     "start_time": "2020-03-31T21:06:17.119144Z"
    }
   },
   "outputs": [],
   "source": [
    "sample['SalePrice'] = np.exp(y_pred)\n",
    "sample.to_csv('./data/predictions_stacking.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dl]",
   "language": "python",
   "name": "conda-env-dl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "185.594px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
